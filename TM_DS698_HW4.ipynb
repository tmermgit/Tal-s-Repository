{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRzd2YlaVu6M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1:\n",
        "Smaller patches:\n",
        "\n",
        "Pros: Capture more fine-grained spatial details and better spatial resolution.\n",
        "\n",
        "Cons: Increase the number of tokens (more computation, slower training, higher memory usage).\n",
        "\n",
        "Larger patches:\n",
        "\n",
        "Pros: Fewer tokens, leading to faster training and lower computational cost.\n",
        "\n",
        "Cons: Lose fine-grained local information and can hurt performance on tasks needing detailed spatial understanding.\n",
        "\n",
        "Trade-off: It's a balance between performance (detail) and efficiency (speed and resources).\n",
        "\n",
        "Q2:\n",
        "CNNs have translation equivariance (due to convolution), local connectivity (focus on nearby pixels), weight sharing (same filters across space). ViTs lack these biases and instead learn from scratch through self-attention.\n",
        "Consequences for CNNs that they require less data to train well due to built-in biases. The consequences for ViTs that they are more flexible and general but need large datasets and compute to learn the same spatial patterns that CNNs get for free.\n",
        "\n",
        "Q3:\n",
        "Self-attention is permutation-invariant. Without positional encoding, the model can't know the order or position of patches. It is implemented through learned or fixed positional encodings.\n",
        "It is added to the patch embeddings before feeding into the transformer encoder. This gives the model spatial awareness.\n",
        "\n",
        "Q4:\n",
        "1. An image encoder (usually a ViT or ResNet).\n",
        "\n",
        "2. A text encoder (usually a Transformer).\n",
        "\n",
        "Each encoder maps its modality into a shared embedding space, so that images and their matching text descriptions lie close together.\n",
        "\n",
        "Q5:\n",
        "CLIP uses contrastive loss (InfoNCE):\n",
        "\n",
        "For a batch of image-text pairs, maximize similarity between matching pairs.\n",
        "\n",
        "Minimize similarity between mismatched pairs (other samples in the batch).\n",
        "\n",
        "It creates a joint embedding space where matched image-text pairs are aligned and separated from others.\n",
        "\n",
        "Q6:\n",
        "For zero-shot classification:\n",
        "\n",
        "-Create textual prompts like “a photo of a cat,” “a photo of a dog,” etc.\n",
        "\n",
        "-Encode each prompt using the text encoder.\n",
        "\n",
        "-Encode the input image and compare it with all text embeddings.\n",
        "\n",
        "-Choose the closest text (class).\n",
        "\n",
        "Prompts act as proxy labels, allowing CLIP to perform classification without training on that specific dataset.\n",
        "\n",
        "Q7:\n",
        "CLIP: Uses softmax-based contrastive loss across the batch.\n",
        "\n",
        "SigLIP: Replaces softmax with sigmoid-based loss, removing the need for cross-sample normalization.\n",
        "\n",
        "Q8:\n",
        "Pros:\n",
        "-Each pair is treated independently which leads to simpler, more stable training.\n",
        "\n",
        "-Better scalability to small batch sizes (important for deployment).\n",
        "\n",
        "Cons:\n",
        "\n",
        "-May lose some benefit of hard negative mining across the batch.\n",
        "\n",
        "Q9:\n",
        "Advantages:\n",
        "\n",
        "-No softmax over batch which is better for small batch inference.\n",
        "\n",
        "-Simpler contrastive loss, which means lower compute and faster inference.\n",
        "\n",
        "-More suitable for edge devices or real-time applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "HtzJ3N3lVvwk"
      }
    }
  ]
}